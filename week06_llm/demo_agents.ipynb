{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNyhjpBPfFHKQ035iQV5D4A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/justheuristic/79d21c8afe45c2ae85b0737ab52a5e29/hf_agents_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let an LLM search the web for you using Transformers Agents\n",
        "_Based on the [original tutorial](https://colab.research.google.com/github/huggingface/cookbook/blob/main/notebooks/en/agents.ipynb) authored by: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
        "\n",
        "This notebook demonstrates how you can use [**Transformers Agents**](https://huggingface.co/docs/transformers/en/agents) to search the web for you.\n",
        "\n",
        "**TL;DR what are agents**? Agents are systems that are powered by an LLM and enable the LLM (with careful prompting and output parsing) to use specific *tools* to solve problems. Tools can be anything: from a calculator to a web search engine, an API or even another LLM. The model is prompted to use a tool by generating a text (e.g. python code) that is then interpretted as a call to a tool. Whatever the tool returns is sent back into the LLM."
      ],
      "metadata": {
        "id": "fi3rd4wkizBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"transformers[agents]\" datasets langchain sentence-transformers faiss-cpu duckduckgo-search openai langchain-community wikipedia --upgrade -q\n",
        "from huggingface_hub import login; login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hux-3EhMk3R4",
        "outputId": "a9705cb1-a346-4261-aa88-5e130d7f659e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.5/389.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's fetch an LLM 'engine' to use those tools:"
      ],
      "metadata": {
        "id": "C3Y4c5LxlKb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# we're gonna use a remote API for now; we'll see local HF agents later\n",
        "llm_engine = transformers.HfApiEngine(\"Qwen/Qwen2.5-72B-Instruct\")"
      ],
      "metadata": {
        "id": "WVtZdHbMk6Ku"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm_engine([{\"role\": \"user\", \"content\": \"Who is the last US president?\"}], stop_sequences=[\"\\n\"])\n",
        "print('\\n\\nOutput (no tools):', output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6GMGjGXvX16",
        "outputId": "ab81c446-70e1-46c5-ce90-10d2450e27d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Output (no tools): The last US president, as of my last update in October 2023, is Joe Biden. He was inaugurated on January 20, 2021, as the 46th President of the United States. If you're reading this after a new president has been inaugurated, please check the most current sources for the latest information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool use: Web Search\n",
        "\n",
        "There are [**many**](https://huggingface.co/docs/transformers/main/en/main_classes/agent) tools available to your models, and you can create new ones as you see fit. This time, we will try one of the more useful ones, a web search, with [duckduckgo.com](https://duckduckgo.com/) as its backend. **Warning:** the DuckDuckGo tool is rate-limited. Calling it too often will result in an exception - just wait for it to cool down."
      ],
      "metadata": {
        "id": "K3K8QlCFqeXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_tool = transformers.agents.search.DuckDuckGoSearchTool()\n",
        "# usage: >>> search_tool.forward(\"The size of your mom\")"
      ],
      "metadata": {
        "id": "-pOUEnwJqZUF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Define a basic agent:__"
      ],
      "metadata": {
        "id": "QCfEXkMlqbAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = transformers.ReactCodeAgent(\n",
        "    tools=[search_tool], llm_engine=llm_engine,\n",
        "    additional_authorized_imports=['math', 'time', 'datetime', 'requests', 're', 'bs4', 'wikipedia']\n",
        ")"
      ],
      "metadata": {
        "id": "girjlNk8rHSf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent.run(\"Who is the latest US president?\")\n",
        "print(\"Output (with search):\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw1Wken4t2Q0",
        "outputId": "12e6761d-7c80-4823-d327-20c9ac9b79f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32;20;1m======== New task ========\u001b[0m\n",
            "\u001b[37;1mWho is the latest US president?\u001b[0m\n",
            "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
            "\u001b[0mThought: I will use the `web_search` tool to find the latest US president.\u001b[0m\n",
            "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
            "\u001b[0m\u001b[38;5;7mresults\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mweb_search\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mquery\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mlatest US president\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
            "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mresults\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
            "\u001b[33;1m====\u001b[0m\n",
            "\u001b[33;1mPrint outputs:\u001b[0m\n",
            "\u001b[32;20m[{'title': 'US president election results 2024 | Live maps, charts and the latest ...', 'href': 'https://www.reuters.com/graphics/USA-ELECTION/RESULTS/zjpqnemxwvx/president/', 'body': 'Updated results from the 2024 election for the US president majority. Reuters live coverage of the 2024 US President, Senate, House and state governors races.'}, {'title': 'Election 2024: Presidential results - CNN', 'href': 'https://www.cnn.com/election/2024/results/president', 'body': 'View maps and real-time results for the 2024 US presidential election matchup between former President Donald Trump and Vice President Kamala Harris. ... View maps and real-time results for the ...'}, {'title': 'US Presidential Election Results 2024 - BBC News', 'href': 'https://www.bbc.com/news/election/2024/us/results', 'body': 'Kamala Harris of the Democrat party has 74,560,685 votes (48.3%) Donald Trump of the Republican party has 77,031,249 votes (49.9%) This map of the US states was filled in as presidential results ...'}, {'title': 'Presidential Election 2024 Live Results: Donald Trump wins - NBC News', 'href': 'https://www.nbcnews.com/politics/2024-elections/president-results', 'body': 'View live election results from the 2024 presidential race as Kamala Harris and Donald Trump face off. See the map of votes by state as results are tallied.'}, {'title': 'Donald Trump defeats Kamala Harris to become the next U.S. president ...', 'href': 'https://www.nbcnews.com/politics/2024-election/trump-wins-election-president-harris-defeat-2024-race-rcna176107', 'body': 'Donald J. Trump, the once and now future president, capped an improbable political comeback by defeating Vice President Kamala Harris on promises to turbocharge the economy and deport undocumented ...'}, {'title': 'US Presidential election: live results | The Economist', 'href': 'https://www.economist.com/interactive/us-2024-election/results/president', 'body': \"Latest update Nov 10th, 13:10 EST | Donald Trump is America's president-elect. All seven swing states were called in his favour, and he improved on his 2020 vote share in nearly 90% of counties.\"}, {'title': 'November 4, 2024 - US election news | CNN Politics', 'href': 'https://www.cnn.com/politics/live-news/2024-election-trump-harris/index.html', 'body': 'Vice President Kamala Harris and former President Donald Trump each need at least 270 electoral votes to win the presidency. Follow here for 2024 presidential election updates, results, analysis ...'}]\n",
            "\u001b[0m\n",
            "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
            "\u001b[0mThought: From the result, we can infer from articles that the latest US president is Donald Trump.\u001b[0m\n",
            "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
            "\u001b[0m\u001b[38;5;7mlatest_president\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mDonald Trump\u001b[39m\u001b[38;5;144m\"\u001b[39m\n",
            "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mlatest_president\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
            "\u001b[33;1m====\u001b[0m\n",
            "\u001b[33;1mPrint outputs:\u001b[0m\n",
            "\u001b[32;20m\u001b[0m\n",
            "\u001b[33;1mLast output from code snippet:\u001b[0m\n",
            "\u001b[32;20mDonald Trump\u001b[0m\n",
            "\u001b[32;20;1mFinal answer:\u001b[0m\n",
            "\u001b[32;20mDonald Trump\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output (with search): Donald Trump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Local LLMs for Agents\n",
        "\n",
        "Below, we define an agent from a `transformers"
      ],
      "metadata": {
        "id": "6oKUIluwv4-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map='auto')\n",
        "# note: you may load larger models with quantization (e.g. load_in_4bit=True or pre-quantized)\n",
        "pipe = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "pipe.generation_config.max_new_tokens = 8192\n",
        "\n",
        "llm_engine = transformers.TransformersEngine(pipe)\n",
        "# usage: >>> result = llm_engine([{\"role\": \"user\", \"content\": \"What is my purpose?\"}], stop_sequences=[\"\\n\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikTHBxnlrRmr",
        "outputId": "a159cee8-b268-4cf3-d432-17d0c9453f8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = transformers.ReactCodeAgent(\n",
        "    tools=[transformers.agents.search.DuckDuckGoSearchTool()],\n",
        "    llm_engine=llm_engine,\n",
        "    additional_authorized_imports=['math', 'time', 'datetime', 'dateutil', 'requests', 're', 'bs4', 'wikipedia']\n",
        ")\n",
        "\n",
        "result = agent.run(\"How old is Donald Trump?\")\n",
        "# note: this may print errors in LLM-generated code - these are fed back into LLM so it may fix them.\n",
        "# note also: this particular LLM solves the problem about half of the time - and often does it after several self-corrections.\n",
        "#            If it tuns out of memory due to too many tokens, just restart runtime (session) and run it again.\n",
        "print(\"Output (local LLM with DuckDuckGo)\", result)"
      ],
      "metadata": {
        "id": "LnALHL0gne2B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see detailed docs / examples at: https://huggingface.co/docs/transformers/en/agents"
      ],
      "metadata": {
        "id": "AJVCD7Prmcj8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
