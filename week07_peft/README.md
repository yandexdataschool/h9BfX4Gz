
__Slides:__ [./lecture_llm_tricks.pdf](./lecture_llm_tricks.pdf)

__Videos (russian):__ [lecture](https://disk.yandex.ru/i/vwudqAlP2gKO-Q) and [seminar](https://disk.yandex.ru/d/L1vjSeaqQfDugw)

__Videos (english):__
- EMNLP tutorial on PEFT by Jonas Pfeifer - https://www.youtube.com/watch?v=KoOlcX3XLd4
- If you don't have 3.5 hours, here's a short version from MunichNLP - https://www.youtube.com/watch?v=StdrAJZsmw4



__Practice assignment:__ [./practice.ipynb](./practice.ipynb) ,  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/2023/week07_peft/practice.ipynb)




__Extra materials (model architecture):__
- "Building ML models like we build open-source software" by Colin Raffel - https://www.youtube.com/watch?v=0oGxT_i7nk8
- Rotary position embeddings explanation from EleutherAI - https://blog.eleuther.ai/rotary-embeddings/
- Group query attention to reduce the memory usage for inference - https://arxiv.org/abs/2305.13245v2
- Gated activations improve transformer (apparently due to divine benevolence) - https://arxiv.org/abs/2002.05202
- as usual, there are dozens of links in the lecture slides (top of this readme)
